{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 25 - Python for Big Data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01 - Big Data and Spark Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Big Data Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### - Explanation of Hadoop, MapReduce, Spark and PySpark \n",
    "### - Local vs Distributed Systems \n",
    "### - Overview of the Hadoop ecosystem \n",
    "### - Detailed overview of Spark \n",
    "### - Set-up of AWS \n",
    "### - Resources and other Spark Options \n",
    "### - JN hands-on code with PySpark and RDD's"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Big Data Intro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Big Data \n",
    "### - So farm we've worked with data that can fit on a local computer, in the scale of 0-8 GB. \n",
    "### - What can we do if we have a larger set of data? \n",
    "###   - Try using an SQL database to move storage onto HDD inseatd of RAM. \n",
    "###   - Use a distributed system, the distributes the data to multiple machines/computer. \n",
    "\n",
    "### Local vs. Distributed\n",
    "### Local -> One machine - Resources of a single machine \n",
    "### Distributed -> Many machines - Access to computational resources across a number of machines connected through \n",
    "###                a network.\n",
    "\n",
    "### Distributed Systems -> We have one machine controlling a distribution of multiple machines. \n",
    "###                     -> For instance , we can have one smaller master node controlling a distributed system \n",
    "###                        of computers. Advantage -> More processing power and more memory. \n",
    "                                                #  -> Fault tolerance -> If one machine fails, the whole network \n",
    "                                                #     the whole network can go on.  \n",
    "###                     -> Easier to scale out to many lower CPU machines than to SCALE UP to a single machine \n",
    "###                        with a high CPU. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Hadoop - Is a way to distribute very large files across multiple machines. \n",
    "### It uses HDFS -> Hadoop Distributed File System \n",
    "### HDFS -> Allows a user to work with large data sets.\n",
    "### HDFS -> Also duplicates blocks of data for fault tolerance.\n",
    "### Also uses MapReduce -> MapReduce allows computations on that data. \n",
    "\n",
    "### Distributed Storage - HDFS -> Consists of 1 Name Node - CPU + RAM and several AND Several N Nodes - CPU + RAM\n",
    "###                            -> HDFS will use blocks of data, with a size of 128 MB. \n",
    "###                            -> Each of these blocks is replicated three times -> Distributed in a way to support \n",
    "###                               Fault tolerance. -> Data is distributed b/w nodes \n",
    "\n",
    "###                            -> Smaller blocks provide more parallelization during processing. \n",
    "###                            -> Multiple copies of a block prevent loss of data due to a failure of a node. \n",
    "\n",
    "### MapReduce -> Is a way of splitting a computation task to a distributed set of files (such as HDFS)\n",
    "###           -> It consists of a Job Tracker and multiple Task Trackers -> Job Trackers sends code to run on\n",
    "###                                                                         task trackers. \n",
    "###                                                                      -> Task trackers allocate CPU and memory\n",
    "###                                                                         for the tasks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Overview \n",
    "### - Spark \n",
    "### - Spark vs. MapReduce \n",
    "### - Spark RDD's \n",
    "### - RDD Operations \n",
    "\n",
    "### ------Spark------------------- \n",
    "### - One of the latest technologies being used to quickly and easily handle big data. \n",
    "### - Ease of use and speed - Created in 2013 \n",
    "### - A flexible alternative to MapReduce -> Can run on top of HDFS and provide additional functionality.\n",
    "### - Spark can also use data stored in a variety of formats:\n",
    "###     - Cassandra\n",
    "###     - AWS S3\n",
    "###     - HDFS\n",
    "\n",
    "### ----Spark vs. MapReduce------- \n",
    "\n",
    "### - We should see Spark as an alternative to MaReduce and not as a replacement for Hadoop.\n",
    "###   -> Wants to provide a comprehensive and unified solution to manage many big data use cases and requirements. \n",
    "### - MapReduce requires files to be stores in HDFS, Spark does not! \n",
    "### - Spark can also perform operations up to 100x faster than MapReduce. \n",
    "\n",
    "### - How does Spark achieve this increase in speed? -> MapReduce writes most data to disk after each map and\n",
    "###   reduce operation. \n",
    "\n",
    "### - Spark keeps most of the data in memory after each transformation and spills over to the disk only if the \n",
    "###   memory has been filled. \n",
    "\n",
    "### -----THE Spark RDD------- \n",
    "\n",
    "### RDD's (Resilient Distributed Dataset) are the core Spark and have four main features: \n",
    "### 1 - Distributed Collection of Data\n",
    "### 2 - Fault-tolerant \n",
    "### 3 - Parallel operation - partioned \n",
    "### 4 - Ability to use many data sources. \n",
    "\n",
    "### Spark operates similarly to any DS.\n",
    "### RDD Objects -> We apply actions and transformations upon them.  \n",
    "### DAG Scheduler - Directed Acyclic Graph -> Allows programmers to develop complex multi-step data Pipelines. \n",
    "\n",
    "### Spark with Python -> Only concerned with RDD objects -> Everything else happens under the hood. \n",
    "\n",
    "### RDD's are immutable, lazily evaluated and cacheable.\n",
    "### There are two types of RDD operations:\n",
    "###    1- Actions \n",
    "###    2- Transformations \n",
    "\n",
    "### Actions -> A. Collect - Return all elements of the RDD as an array at the driver program.\n",
    "###         -> B. Count   - Return the number of elements in the RDD\n",
    "###         -> C. First   - Returns the first element of the RDD \n",
    "###         -> D. Take    - Returns an array with the first n elements of the RDD\n",
    "### These actions will look like methods when programmed with Python  \n",
    "\n",
    "### Transformations -> A. Filter  -> Applies each function to each element and returns elements \n",
    "                                ##   evaluate to True.\n",
    "    \n",
    "###                 -> B. Map     -> Transforms each elements and preserves number of elements.\n",
    "###                 -> C. Flatmap -> Transforms each element in 0-N elements. -> CHnages number of elements \n",
    "\n",
    "### Pair RDD's -> Often, RDD's will be holding their values in tuples as a key-value pair. \n",
    "###            -> This offers better partitioning of data and leads to functionality based on reduction. \n",
    "\n",
    "### Reduce and ReduceByKey ->\n",
    "\n",
    "### Reduce() -> Will aggregate RDD elements using a function that returns a single element. \n",
    "\n",
    "### ReduceByKey() -> A action that will aggregate Pair RDD elements using a function that returns a Pair RDD \n",
    "### These ideas are similar to the Pandas GroupBy Operation "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
