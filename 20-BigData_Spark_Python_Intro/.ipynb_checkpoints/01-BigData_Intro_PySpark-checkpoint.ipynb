{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 25: Python for Big Data : Intro to Spark and Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Big Data Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### - Explanation of Hadoop, MapReduce, Spark and PySpark \n",
    "### - Local vs Distributed Systems \n",
    "### - Overview of the Hadoop ecosystem \n",
    "### - Detailed overview of Spark \n",
    "### - Set-up of AWS \n",
    "### - Resources and other Spark Options \n",
    "### - JN hands-on code with PySpark and RDD's"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Big Data Intro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Big Data \n",
    "### - So farm we've worked with data that can fit on a local computer, in the scale of 0-8 GB. \n",
    "### - What can we do if we have a larger set of data? \n",
    "###   - Try using an SQL database to move storage onto HDD inseatd of RAM. \n",
    "###   - Use a distributed system, the distributes the data to multiple machines/computer. \n",
    "\n",
    "### Local vs. Distributed\n",
    "### Local -> One machine - Resources of a single machine \n",
    "### Distributed -> Many machines - Access to computational resources across a number of machines connected through \n",
    "###                a network.\n",
    "\n",
    "### Distributed Systems -> We have one machine controlling a distribution of multiple machines. \n",
    "###                     -> For instance , we can have one smaller master node controlling a distributed system \n",
    "###                        of computers. Advantage -> More processing power and more memory. \n",
    "                                                #  -> Fault tolerance -> If one machine fails, the whole network \n",
    "                                                #     the whole network can go on.  \n",
    "###                     -> Easier to scale out to many lower CPU machines than to SCALE UP to a single machine \n",
    "###                        with a high CPU. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Hadoop - Is a way to distribute very large files across multiple machines. \n",
    "### It uses HDFS -> Hadoop Distributed File System \n",
    "### HDFS -> Allows a user to work with large data sets.\n",
    "### HDFS -> Also duplicates blocks of data for fault tolerance.\n",
    "### Also uses MapReduce -> MapReduce allows computations on that data. \n",
    "\n",
    "### Distributed Storage - HDFS -> Consists of 1 Name Node - CPU + RAM and several AND Several N Nodes - CPU + RAM\n",
    "###                            -> HDFS will use blocks of data, with a size of 128 MB. \n",
    "###                            -> Each of these blocks is replicated three times -> Distributed in a way to support \n",
    "###                               Fault tolerance. -> Data is distributed b/w nodes \n",
    "\n",
    "###                            -> Smaller blocks provide more parallelization during processing. \n",
    "###                            -> Multiple copies of a block prevent loss of data due to a failure of a node. \n",
    "\n",
    "### MapReduce -> Is a way of splitting a computation task to a distributed set of files (such  as HDFS)\n",
    "###           -> It consists of a Job Tracker and multiple Task Trackers -> Job Trackers sends code to run on\n",
    "###                                                                         task trackers. \n",
    "###                                                                      -> Task trackers allocate CPU and memory\n",
    "###                                                                         for the tasks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
